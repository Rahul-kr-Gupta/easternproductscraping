# Daily Automated Scraper - Deployment Guide

## ğŸš€ Automated Daily Scraping at 4pm AEST

Your scraper is now configured for **daily automated execution** at 4pm AEST.

## ğŸ“‹ What the Automation Does

Every day at 4pm AEST, the system will automatically:

1. **Login to Pronto website** (`login_and_save_cookies_.py`)
   - Authenticates with your credentials
   - Saves fresh cookies to `pronto_cookies.json`

2. **Scrape all products** (`scrape_products_with_cookies.py`)
   - Loads the saved cookies
   - Scrapes all 5,229 products from the CSV
   - Auto-refreshes cookies every 10 minutes during the 5-6 hour scrape
   - Saves data to `eastern_scraped_data.csv`

3. **Generate daily report**
   - Summary of products scraped
   - Success/failure status
   - Timestamp of completion

## âš™ï¸ Deployment Configuration

The deployment has been configured as a **scheduled job** with:

- **Script**: `daily_scraper.py` (wrapper that runs both scripts in sequence)
- **Type**: Scheduled deployment
- **Schedule**: Daily at 4pm AEST (6am UTC)
- **Credentials**: Automatically uses your stored secrets (EASTERN_USERNAME, EASTERN_PASSWORD)

## ğŸ“… Setting the Schedule

To complete the deployment and set the 4pm AEST schedule:

1. Click the **"Deploy"** button in Replit
2. Choose **"Scheduled"** deployment type
3. Set the schedule using cron expression: `0 6 * * *`
   - This runs at 6:00 AM UTC = 4:00 PM AEST (non-DST)
   - For AEDT (daylight saving): `0 5 * * *` = 4:00 PM AEDT

### Cron Expression Explained
```
0 6 * * *
â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€ Day of week (0-7, both 0 and 7 are Sunday)
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€ Month (1-12)
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ Day of month (1-31)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hour (0-23) - 6 = 6am UTC = 4pm AEST
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minute (0-59) - 0 = on the hour
```

## ğŸ” Security Features

âœ… **Credentials stored securely** in Replit Secrets
âœ… **Never exposed** in code or logs
âœ… **Automatic cookie refresh** every 10 minutes
âœ… **Session recovery** if authentication expires

## ğŸ“Š Monitoring the Daily Run

### View Deployment Logs
In Replit's Deployment section, you can:
- See when each run starts and completes
- View real-time progress
- Check for any errors
- Download the scraped CSV file

### Check Output File
After each run, check `eastern_scraped_data.csv` to see:
- All scraped products
- Updated prices and inventory
- Timestamp of when each product was scraped

## ğŸ”„ What Happens During Each Run

```
4:00 PM AEST - Scheduled job starts
  â†“
Step 1: Login (30 seconds)
  â”œâ”€ Opens browser
  â”œâ”€ Logs into Pronto website
  â””â”€ Saves cookies to pronto_cookies.json
  â†“
Step 2: Scrape Products (5-6 hours)
  â”œâ”€ Loads cookies
  â”œâ”€ Scrapes all 5,229 products
  â”œâ”€ Auto-refreshes cookies every 10 minutes
  â””â”€ Saves to eastern_scraped_data.csv
  â†“
10:00 PM AEST - Job completes
  â”œâ”€ Generates summary report
  â””â”€ Updates deployment logs
```

## ğŸ“ Output Files Generated Daily

1. **`eastern_scraped_data.csv`** - Complete product data
   - All 5,229 products
   - 11 fields per product
   - Fresh pricing and inventory status

2. **`pronto_cookies.json`** - Session cookies
   - Updated daily
   - Valid for 24+ hours
   - Used for authenticated scraping

3. **Deployment Logs** - Execution details
   - Start/end times
   - Success/failure status
   - Error messages (if any)

## ğŸ› ï¸ Customization Options

### Change Schedule Time
Edit the cron expression in deployment settings:
- `0 8 * * *` = 6pm AEST
- `0 4 * * *` = 2pm AEST
- `0 22 * * 1-5` = 8am AEST, weekdays only

### Change Scope
Edit `daily_scraper.py` line 56:
```python
# Current: Scrapes all products
"python scrape_products_with_cookies.py full"

# Alternative: Scrape first 100 products only
"python scrape_products_with_cookies.py test 100"
```

## âš¡ Built-in Features

âœ… **10-minute cookie refresh** - Prevents session timeouts
âœ… **Automatic retry** - Re-authenticates if cookies expire
âœ… **Resume capability** - Can recover from interruptions
âœ… **Progress tracking** - Know exactly how many products scraped
âœ… **Error resilience** - Continues even if individual products fail

## ğŸ“§ Next Steps

1. **Deploy Now**: Click "Deploy" in Replit
2. **Set Schedule**: Use cron expression `0 6 * * *`
3. **Monitor First Run**: Check logs to ensure it works
4. **Daily Access**: Fresh data available every evening

## ğŸ¯ Benefits of Daily Automation

- **Fresh Data**: Always have up-to-date product information
- **No Manual Work**: Fully automated, no intervention needed
- **Consistent Schedule**: Runs at same time every day
- **Historical Tracking**: Save each day's CSV for trend analysis
- **Reliable**: Auto-recovery from errors and session timeouts

---

**Your daily scraper is ready to deploy! Just click the Deploy button and set the schedule.** ğŸš€
